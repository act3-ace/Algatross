{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PZ seed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import seed_test, parallel_seed_test\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "seed = 1000\n",
    "env_fn = simple_tag_v3.parallel_env\n",
    "parallel_seed_test(env_fn, num_cycles=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algatross.agents.on_policy.ppo import TorchPPOAgent\n",
    "\n",
    "env = env_fn()\n",
    "agent = TorchPPOAgent(\n",
    "    obs_space=env.observation_space(\"agent_0\"),\n",
    "    act_space=env.action_space(\"agent_0\"),\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, _info = env.reset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "actions: dict[str, torch.Tensor | np.ndarray] = {}\n",
    "logits: dict[str, torch.Tensor | np.ndarray] = {}\n",
    "values: dict[str, torch.Tensor | np.ndarray] = {}\n",
    "logp: dict[str, torch.Tensor | np.ndarray] = {}\n",
    "\n",
    "agent_id = \"agent_0\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    obs = next_obs\n",
    "    torch_obs = torch.from_numpy(obs[\"agent_0\"]).unsqueeze(0)\n",
    "    ag_logits = agent.actor(torch_obs)\n",
    "    ag_actions, ag_values, dist = agent.get_action_and_value(torch_obs, logits=ag_logits)\n",
    "    ag_logp = dist.log_prob(ag_actions)\n",
    "    actions[agent_id] = ag_actions.numpy()[0]\n",
    "    logits[agent_id] = ag_logits.numpy()\n",
    "    values[agent_id] = ag_values.numpy()\n",
    "    logp[agent_id] = ag_logp.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.6734749 , -0.00259911,  0.51732236,\n",
       "       -0.7848932 ,  1.2760446 ,  0.49880967,  1.6354212 , -0.87701476,\n",
       "        0.63363993, -0.29134926,  1.4245683 , -0.89229226], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[\"agent_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config -> rollout with runner from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from algatross.utils.parsers.yaml_loader import load_config\n",
    "os.chdir(\"/home/wgar/mo-marl/\")\n",
    "seed = 1000\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration: /home/wgar/mo-marl/config/simple_tag/test_algatross.yml\n"
     ]
    }
   ],
   "source": [
    "config_file = \"/home/wgar/mo-marl/config/simple_tag/test_algatross.yml\"  # noqa: PLR2004\n",
    "print(f\"Using configuration: {config_file}\")\n",
    "\n",
    "config = load_config(config_file)\n",
    "# grab one of the islands\n",
    "island_config = config['islands'][0]\n",
    "# get the UDP constructor and data\n",
    "udp_constructor = island_config['problem_constructor'].config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_tag_v3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'adversary_0': array([ 0.        ,  0.        ,  0.04277148,  0.2076837 ,  0.5068985 ,\n",
       "         -0.6606129 , -0.60102814,  0.66350836, -0.10088788, -0.8011878 ,\n",
       "          0.01474658, -0.8256111 , -0.47968027,  0.2996794 ,  0.06057208,\n",
       "          0.51976043,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        dtype=float32),\n",
       "  'adversary_1': array([ 0.        ,  0.        , -0.05811641, -0.59350413,  0.6077864 ,\n",
       "          0.1405749 , -0.50014025,  1.4646962 ,  0.10088788,  0.8011878 ,\n",
       "          0.11563446, -0.02442333, -0.3787924 ,  1.1008673 ,  0.16145995,\n",
       "          1.3209482 ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        dtype=float32),\n",
       "  'adversary_2': array([ 0.        ,  0.        ,  0.05751805, -0.61792743,  0.49215195,\n",
       "          0.16499823, -0.6157747 ,  1.4891195 , -0.01474658,  0.8256111 ,\n",
       "         -0.11563446,  0.02442333, -0.49442685,  1.1252905 ,  0.0458255 ,\n",
       "          1.3453716 ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        dtype=float32),\n",
       "  'agent_0': array([ 0.        ,  0.        , -0.4369088 ,  0.5073631 ,  0.9865788 ,\n",
       "         -0.96029234, -0.12134785,  0.36382896,  0.47968027, -0.2996794 ,\n",
       "          0.3787924 , -1.1008673 ,  0.49442685, -1.1252905 ,  0.5402523 ,\n",
       "          0.22008105,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        dtype=float32),\n",
       "  'agent_1': array([ 0.        ,  0.        ,  0.10334355,  0.7274442 ,  0.44632643,\n",
       "         -1.1803733 , -0.66160023,  0.1437479 , -0.06057208, -0.51976043,\n",
       "         -0.16145995, -1.3209482 , -0.0458255 , -1.3453716 , -0.5402523 ,\n",
       "         -0.22008105,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        dtype=float32)},\n",
       " {'adversary_0': {},\n",
       "  'adversary_1': {},\n",
       "  'adversary_2': {},\n",
       "  'agent_0': {},\n",
       "  'agent_1': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from supersuit.multiagent_wrappers import pad_action_space_v0, pad_observations_v0\n",
    "\n",
    "seed = 1000\n",
    "raw_env = udp_constructor['env_constructor_data'].construct()\n",
    "print(raw_env)\n",
    "env = pad_observations_v0(pad_action_space_v0(raw_env))\n",
    "env.reset(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'algatross.environments.mpe.simple_tag.MPESimpleTagRunner'>\n",
      "{'rollout_kwargs': {'batch_size': 300, 'gamma': 0.85, 'gae_lambda': 0.85}, 'train_kwargs': {'device': 'cpu', 'sgd_minibatch_size': 30, 'num_sgd_iter': 20}}\n",
      "Seeding runner with 1000!\n"
     ]
    }
   ],
   "source": [
    "## create a env runner\n",
    "print(udp_constructor['cleanrl_trainer_constructor_data'].constructor)\n",
    "print(udp_constructor['cleanrl_trainer_constructor_data'].config)\n",
    "\n",
    "runner = udp_constructor['cleanrl_trainer_constructor_data'].constructor(\n",
    "    seed=seed,\n",
    "    env=env,\n",
    "    n_envs=1,\n",
    "    train_config=udp_constructor['cleanrl_trainer_constructor_data'].config,\n",
    "    evaluate_config=udp_constructor['cleanrl_evaluator_constructor_data'].config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'algatross.agents.on_policy.ppo.TorchPPOAgent'>\n",
      "{'critic_outs': 1, 'shared_encoder': False, 'free_log_std': True, 'entropy_coeff': 0.0, 'kl_target': 0.2, 'kl_coeff': 0.2, 'vf_coeff': 1.0, 'seed': 1000, 'logp_clip_param': 0.2, 'vf_clip_param': None, 'optimizer_class': <class 'torch.optim.adam.Adam'>, 'optimizer_kwargs': {'lr': 0.0003}}\n"
     ]
    }
   ],
   "source": [
    "from algatross.utils.types import AgentID, ConstructorData, MOAIMIslandInhabitant, OptimizationTypeEnum, TeammateID, IslandID, TeamID\n",
    "from typing import Callable\n",
    "import functools\n",
    "\n",
    "## UDP fundamentals\n",
    "print(udp_constructor['cleanrl_agent_constructor_data'].constructor)\n",
    "print(udp_constructor['cleanrl_agent_constructor_data'].config)\n",
    "\n",
    "fitness_metric_keys = udp_constructor['fitness_metric_keys']\n",
    "fitness_metric_optimization_type = udp_constructor['fitness_metric_optimization_type']\n",
    "fitness_multiplier = udp_constructor['fitness_multiplier']\n",
    "fitness_reduce_fn = udp_constructor['fitness_reduce_fn']\n",
    "\n",
    "if isinstance(fitness_metric_optimization_type, str):\n",
    "    fitness_opt_enum = [OptimizationTypeEnum(fitness_metric_optimization_type.lower())] * len(fitness_metric_keys)\n",
    "else:\n",
    "    # use zip(..., strict=True) to ensure equal length iterables\n",
    "    fitness_opt_enum = [\n",
    "        OptimizationTypeEnum(fo_type.lower())\n",
    "        for _, fo_type in zip(fitness_metric_keys, fitness_metric_optimization_type, strict=True)\n",
    "    ]\n",
    "\n",
    "_fitness_metric_opt_enum = fitness_opt_enum\n",
    "_fitness_sign = np.array([1 if opt_type == OptimizationTypeEnum.MAX else -1 for opt_type in fitness_opt_enum])\n",
    "_fitness_reduce_fn: Callable = (\n",
    "    functools.partial(  # type: ignore[assignment]\n",
    "        functools.partial(lambda x, y, *x_args, **x_kwargs: getattr(y, x)(*x_args, **x_kwargs), fitness_reduce_fn),\n",
    "        axis=-1,\n",
    "    )\n",
    "    if isinstance(fitness_reduce_fn, str)\n",
    "    else fitness_reduce_fn\n",
    ")\n",
    "\n",
    "if fitness_multiplier is None:\n",
    "    fitness_multiplier = np.array([1.0] * len(fitness_metric_keys), dtype=np.float32)\n",
    "\n",
    "fitness_multiplier = np.array([0.0 if fm is None else fm for fm in fitness_multiplier], dtype=np.float32)\n",
    "fitness_multiplier = fitness_multiplier * _fitness_sign\n",
    "\n",
    "\n",
    "### agents and team composition\n",
    "agent_ids = env.possible_agents\n",
    "training_agents = udp_constructor['training_agents']\n",
    "\n",
    "try:\n",
    "    ally_agents = set()\n",
    "    ally_teams = udp_constructor['ally_teams']\n",
    "except KeyError:\n",
    "    ally_teams = {\"allies_0\": list(training_agents)}\n",
    "\n",
    "\n",
    "try:\n",
    "    opponent_agents = set()\n",
    "    opponent_teams = udp_constructor['opponent_teams']\n",
    "except KeyError:\n",
    "    opponent_teams = {}\n",
    "\n",
    "\n",
    "try:\n",
    "    neutral_agents = set()\n",
    "    neutral_teams = udp_constructor['neutral_teams']\n",
    "except KeyError:\n",
    "    neutral_teams = {}\n",
    "\n",
    "\n",
    "for team in ally_teams.values():\n",
    "    ally_agents.update(set(team))\n",
    "for team in opponent_teams.values():\n",
    "    opponent_agents.update(set(team))\n",
    "for team in neutral_teams.values():\n",
    "    neutral_agents.update(set(team))\n",
    "\n",
    "\n",
    "## UDP.init_agents()\n",
    "_agent_map = {\n",
    "    agent_id: udp_constructor['cleanrl_agent_constructor_data'].constructor(\n",
    "        agent_id=agent_id,\n",
    "        obs_space=env.observation_space(agent_id),\n",
    "        act_space=env.action_space(agent_id),\n",
    "        **udp_constructor['cleanrl_agent_constructor_data'].config\n",
    "    ) for agent_id in agent_ids\n",
    "}\n",
    "_solution_dim = {\n",
    "    agent_id: np.prod(agent.flat_parameters.shape) for agent_id, agent in _agent_map.items() if agent_id in training_agents\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_0', 'agent_1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ally_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create archive with the following parameter\n",
      "{'measure_dim': 4, 'k_neighbors': 5, 'novelty_threshold': 2, 'qd_score_offset': -500, 'extra_fields': {'trajectory': ((), <class 'numpy.object_'>)}, 'solution_dim': 11205, 'seed': 1000}\n"
     ]
    }
   ],
   "source": [
    "### population and pyribs archive create\n",
    "###\n",
    "import dataclasses\n",
    "from algatross.algorithms.genetic.mo_aim.population import MOAIMIslandPopulationConfig, EmitterBase\n",
    "from algatross.utils.types import AgentID, ConstructorData, MOAIMIslandInhabitant, OptimizationTypeEnum, TeammateID, IslandID, TeamID\n",
    "from algatross.utils.random import get_generators\n",
    "from contextlib import suppress\n",
    "from typing import Sequence\n",
    "from itertools import chain, cycle\n",
    "\n",
    "island_id: IslandID = 0\n",
    "isl_pop_config = MOAIMIslandPopulationConfig(**island_config[\"population_constructor\"].config)\n",
    "\n",
    "_isl_pop_numpy_generator, _ = get_generators(seed=seed)\n",
    "\n",
    "## MOAIMIslandPopulation setup()\n",
    "pop_solution_dim = next(iter((_solution_dim if isl_pop_config.solution_dim is None else isl_pop_config.solution_dim).values()))\n",
    "\n",
    "# copy instead of pass by reference because jupyter\n",
    "archive_config = (\n",
    "    isl_pop_config.archive_config.copy() if isinstance(isl_pop_config.archive_config, dict) else dataclasses.asdict(isl_pop_config.archive_config).copy()\n",
    ")\n",
    "extra_fields = archive_config.get(\"extra_fields\", {})\n",
    "extra_fields[\"trajectory\"] = ((), np.object_)\n",
    "archive_config[\"extra_fields\"] = extra_fields\n",
    "archive_config[\"solution_dim\"] = pop_solution_dim\n",
    "archive_config[\"seed\"] = seed\n",
    "\n",
    "result_archive = None\n",
    "\n",
    "###### emitter\n",
    "emitter_config = (\n",
    "    isl_pop_config.emitter_config if isinstance(isl_pop_config.emitter_config, dict) else dataclasses.asdict(isl_pop_config.emitter_config)\n",
    ")\n",
    "emitter_config[\"seed\"] = seed\n",
    "if emitter_config.get(\"initial_solutions\") is None:\n",
    "    emitter_config[\"initial_solutions\"] = _isl_pop_numpy_generator.normal(\n",
    "        0,\n",
    "        1,\n",
    "        (emitter_config[\"batch_size\"], next(iter(_solution_dim.values()))),\n",
    "    )\n",
    "\n",
    "\n",
    "##### random emitter\n",
    "random_emitter_config = (\n",
    "    isl_pop_config.random_emitter_config\n",
    "    if isinstance(isl_pop_config.random_emitter_config, dict)\n",
    "    else dataclasses.asdict(isl_pop_config.random_emitter_config)\n",
    ")\n",
    "random_emitter_config[\"seed\"] = seed\n",
    "if random_emitter_config.get(\"initial_solutions\") is None:\n",
    "    random_emitter_config[\"initial_solutions\"] = _isl_pop_numpy_generator.normal(\n",
    "        0,\n",
    "        1,\n",
    "        (random_emitter_config[\"batch_size\"], next(iter(_solution_dim.values()))),\n",
    "    )\n",
    "\n",
    "if isl_pop_config.use_result_archive:\n",
    "    result_archive_config = (\n",
    "        isl_pop_config.result_archive_config\n",
    "        if isinstance(isl_pop_config.result_archive_config, dict)\n",
    "        else dataclasses.asdict(isl_pop_config.result_archive_config)\n",
    "    )\n",
    "    extra_fields = result_archive_config.get(\"extra_fields\", {})\n",
    "    extra_fields[\"trajectory\"] = ((), np.object_)\n",
    "    result_archive_config[\"extra_fields\"] = extra_fields\n",
    "    result_archive_config[\"solution_dim\"] = pop_solution_dim\n",
    "    result_archive_config[\"seed\"] = seed\n",
    "\n",
    "# default to 1 sample per migrant if set to 0\n",
    "isl_pop_config.max_samples_per_migrant = isl_pop_config.max_samples_per_migrant or 1\n",
    "\n",
    "## setup_qd()\n",
    "# create the archive and possibly result archive\n",
    "print(\"Create archive with the following parameter\")\n",
    "print(archive_config)\n",
    "archive = isl_pop_config.archive_base_class(**archive_config)\n",
    "\n",
    "if isl_pop_config.use_result_archive:\n",
    "    result_archive_config = (\n",
    "        isl_pop_config.result_archive_config\n",
    "        if isinstance(isl_pop_config.result_archive_config, dict)\n",
    "        else dataclasses.asdict(isl_pop_config.result_archive_config)\n",
    "    )\n",
    "    result_archive = isl_pop_config.result_archive_base_class(**result_archive_config)\n",
    "\n",
    "# create the emitters\n",
    "# TODO: currently always length one\n",
    "emitters = [\n",
    "    isl_pop_config.emitter_base_class(archive, **emitter_config),\n",
    "]\n",
    "random_emitter = isl_pop_config.random_emitter_base_class(\n",
    "    archive,\n",
    "    **random_emitter_config,\n",
    ")\n",
    "\n",
    "# create the scheduler\n",
    "scheduler = isl_pop_config.scheduler_base_class(\n",
    "    archive,\n",
    "    emitters,\n",
    "    result_archive=result_archive,\n",
    "    **isl_pop_config.scheduler_config,\n",
    ")\n",
    "\n",
    "def random_emitter_generator():\n",
    "    while True:\n",
    "        yield _isl_pop_numpy_generator.choice(emitters)\n",
    "\n",
    "def get_random_teammates(n_teammates: int = 0) -> list[np.ndarray]:\n",
    "    return [random_emitter.ask() for _ in range(n_teammates)]\n",
    "\n",
    "def build_batch(\n",
    "    emitter: EmitterBase,\n",
    "    batch_size: int | None,\n",
    "    team_id: int = 0,\n",
    "    do_batched: bool = False,\n",
    "    teammates: Sequence[np.ndarray] | None = None,\n",
    "    names: Sequence[AgentID] | None = None,\n",
    ") -> dict[TeamID, dict[AgentID, MOAIMIslandInhabitant]]:\n",
    "    teams = {}\n",
    "    name = iter(names or [f\"agent_{idx}\" for idx in range(1 + len(teammates or []))])\n",
    "    genomes = emitter.ask()\n",
    "    for genome in genomes:\n",
    "        ind = MOAIMIslandInhabitant(\n",
    "            name=next(name),\n",
    "            team_id=team_id,\n",
    "            inhabitant_id=0,\n",
    "            genome=genome,\n",
    "            island_id=island_id,\n",
    "            current_island_id=island_id,\n",
    "            conspecific_utility_dict=dict(zip(fitness_metric_keys, fitness_multiplier, strict=True)),\n",
    "        )\n",
    "        team = {ind.name: ind}\n",
    "        if teammates is not None:\n",
    "            team.update(\n",
    "                {\n",
    "                    ally.name: ally\n",
    "                    for ally in [\n",
    "                        MOAIMIslandInhabitant(\n",
    "                            name=next(name),\n",
    "                            team_id=team_id,\n",
    "                            inhabitant_id=tm_id + 1,\n",
    "                            genome=t,\n",
    "                            island_id=island_id,\n",
    "                            current_island_id=island_id,\n",
    "                            conspecific_utility_dict=dict(\n",
    "                                zip(fitness_metric_keys, fitness_multiplier, strict=True),\n",
    "                            ),\n",
    "                        )\n",
    "                        for tm_id, t in enumerate(teammates)\n",
    "                    ]\n",
    "                },\n",
    "            )\n",
    "        teams[team_id] = team\n",
    "        name = iter(names or [f\"agent_{idx}\" for idx in range(1 + len(teammates or []))])\n",
    "        team_id += 1\n",
    "        if do_batched and team_id >= batch_size:\n",
    "            break\n",
    "    return teams\n",
    "\n",
    "\n",
    "def get_teams_for_training(\n",
    "        batch_size: int | None = None,\n",
    "        randomized: bool = True\n",
    "    ):\n",
    "    teams: dict[TeamID, list] = {}\n",
    "    do_batched = batch_size is not None\n",
    "    n_emitters = len(emitters)\n",
    "    names = sorted(ally_agents)\n",
    "    if randomized:\n",
    "        batch_size = n_emitters\n",
    "        emitter = random_emitter_generator()\n",
    "    elif do_batched:\n",
    "        emitter = cycle(emitters)  # type: ignore[assignment]\n",
    "    else:\n",
    "        emitter = iter(emitters)  # type: ignore[assignment]\n",
    "\n",
    "    # convenience function for checking if while look should continue\n",
    "    def should_continue(teams, do_batched, n_emitters=n_emitters, batch_size=batch_size):\n",
    "        if do_batched:\n",
    "            return len(teams) < batch_size\n",
    "        return len(teams) < n_emitters\n",
    "\n",
    "    loops = 0\n",
    "    with suppress(StopIteration):\n",
    "        while should_continue(teams, do_batched, n_emitters):\n",
    "            loops += 1\n",
    "            teammates = (\n",
    "                get_random_teammates(isl_pop_config.team_size - len(training_agents)) if isl_pop_config.team_size else []\n",
    "            )\n",
    "            print('teammates', teammates)\n",
    "            teams.update(\n",
    "                build_batch(  # type: ignore[arg-type]\n",
    "                    emitter=next(emitter),\n",
    "                    batch_size=batch_size,\n",
    "                    team_id=len(teams),\n",
    "                    do_batched=do_batched,\n",
    "                    teammates=teammates,\n",
    "                    names=names,\n",
    "                ),\n",
    "            )\n",
    "            loops += 1\n",
    "    return teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teammates [array([[-0.1058925 ,  0.68494484,  0.71998265, ..., -0.69250414,\n",
      "         0.03490563,  0.51367037]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'agent_0': MOAIMIslandInhabitant(name='agent_0', team_id=0, inhabitant_id=0, island_id=0, current_island_id=0, genome=array([-0.32133021, -0.48566148,  1.68005813, ...,  1.34315646,\n",
       "         -0.36653313,  0.34672924]), conspecific_utility_dict={'tag_score': 0.0, 'minimum_ally_speed': -0.25, 'minimum_adversary_speed': 0.5, 'closest_ally_distance': -0.25, 'closest_adversary_distance': 1.0, 'closest_landmark_distance': -1.0, 'boundary_penalty': 1.0}, db_hash=UUID('b435ed27-34ba-4e18-8aa0-9fee90cdd4eb')),\n",
       "  'agent_1': MOAIMIslandInhabitant(name='agent_1', team_id=0, inhabitant_id=1, island_id=0, current_island_id=0, genome=array([[-0.1058925 ,  0.68494484,  0.71998265, ..., -0.69250414,\n",
       "           0.03490563,  0.51367037]]), conspecific_utility_dict={'tag_score': 0.0, 'minimum_ally_speed': -0.25, 'minimum_adversary_speed': 0.5, 'closest_ally_distance': -0.25, 'closest_adversary_distance': 1.0, 'closest_landmark_distance': -1.0, 'boundary_penalty': 1.0}, db_hash=UUID('ee9724e4-cb60-48e9-8e74-4a67d80b1ff9'))}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treset env with params {'seed': 1000, 'tape_index': 0, 'env_rank': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 18:50:03,489\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'agent/adversary_0/rollout/returns': 6.6666665,\n",
       "             'agent/adversary_1/rollout/returns': 6.6666665,\n",
       "             'agent/adversary_2/rollout/returns': 6.6666665,\n",
       "             'agent/agent_0/training/actor_loss': array([0.24308524, 0.23575436, 0.22775751, 0.22112583, 0.21621878,\n",
       "                    0.21186645, 0.20772243, 0.203897  , 0.20057376, 0.19804901,\n",
       "                    0.19608526, 0.1955801 , 0.19495025, 0.19422422, 0.19351475,\n",
       "                    0.19284995, 0.19210418, 0.1912758 , 0.19043167, 0.18975091],\n",
       "                   dtype=float32),\n",
       "             'agent/agent_0/training/critic_loss': array([122.07038 , 120.478355, 118.89169 , 117.310905, 115.73661 ,\n",
       "                    114.16967 , 112.61091 , 111.061195, 109.52139 , 107.99229 ,\n",
       "                    106.47454 , 104.96884 , 103.47575 , 101.99589 , 100.52983 ,\n",
       "                     99.07817 ,  97.64146 ,  96.22032 ,  94.81526 ,  93.42678 ],\n",
       "                   dtype=float32),\n",
       "             'agent/agent_0/training/entropy': array([0.29817614, 0.2981474 , 0.2979261 , 0.29729035, 0.29680648,\n",
       "                    0.29611152, 0.29519385, 0.29415035, 0.29316747, 0.29260734,\n",
       "                    0.29269662, 0.29428613, 0.2972879 , 0.30157182, 0.30700156,\n",
       "                    0.31300852, 0.31961218, 0.32676628, 0.3343934 , 0.34155777],\n",
       "                   dtype=float32),\n",
       "             'agent/agent_0/training/entropy_coeff': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                    0., 0., 0.], dtype=float32),\n",
       "             'agent/agent_0/training/grad_norm': array([249.03548, 249.03606, 249.0366 , 249.03717, 249.03781, 249.0384 ,\n",
       "                    249.03905, 249.03978, 249.04044, 249.04118, 249.04193, 249.0427 ,\n",
       "                    249.04349, 249.04433, 249.04518, 249.04614, 249.0471 , 249.04803,\n",
       "                    249.04901, 249.05   ], dtype=float32),\n",
       "             'agent/agent_0/training/kl_coeff': array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], dtype=float32),\n",
       "             'agent/agent_0/training/kl_loss': array([-9.6895558e-09,  7.1297813e-04,  2.9065751e-03,  6.6672168e-03,\n",
       "                     1.1642937e-02,  1.7627737e-02,  2.4483632e-02,  3.2042246e-02,\n",
       "                     4.0093713e-02,  4.8205305e-02,  5.6128673e-02,  6.2581383e-02,\n",
       "                     6.7643881e-02,  7.1473628e-02,  7.4260719e-02,  7.5897954e-02,\n",
       "                     7.6631315e-02,  7.6684944e-02,  7.6268360e-02,  7.5095400e-02],\n",
       "                   dtype=float32),\n",
       "             'agent/agent_0/training/total_loss': array([122.31345 , 120.714264, 119.120026, 117.533356, 115.95518 ,\n",
       "                    114.385056, 112.82352 , 111.2715  , 109.72999 , 108.199974,\n",
       "                    106.68188 , 105.17694 , 103.684235, 102.20443 , 100.738205,\n",
       "                     99.28619 ,  97.8489  ,  96.42693 ,  95.020935,  93.63156 ],\n",
       "                   dtype=float32),\n",
       "             'agent/agent_0/training/vf_explained_var': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                    0., 0., 0.], dtype=float32),\n",
       "             'agent/agent_0': {'tag_score': -0.0,\n",
       "              'minimum_ally_speed': 0.6769992709159851,\n",
       "              'minimum_adversary_speed': 0.18167078495025635,\n",
       "              'closest_ally_distance': -0.0,\n",
       "              'closest_adversary_distance': 20.60738182067871,\n",
       "              'closest_landmark_distance': -24.191078186035156,\n",
       "              'boundary_penalty': -29.014312744140625},\n",
       "             'tag_score': 0.0,\n",
       "             'minimum_ally_speed': 0.6769992709159851,\n",
       "             'minimum_adversary_speed': 0.18167078495025635,\n",
       "             'closest_ally_distance': 0.0,\n",
       "             'closest_adversary_distance': 20.60738182067871,\n",
       "             'closest_landmark_distance': -24.191078186035156,\n",
       "             'boundary_penalty': -29.014312744140625})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tag_score 0.0\n",
      "agent/agent_0/training/total_loss [122.31345  120.714264 119.120026 117.533356 115.95518  114.385056\n",
      " 112.82352  111.2715   109.72999  108.199974 106.68188  105.17694\n",
      " 103.684235 102.20443  100.738205  99.28619   97.8489    96.42693\n",
      "  95.020935  93.63156 ]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Any, Literal\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from algatross.utils.merge_dicts import flatten_dicts\n",
    "\n",
    "\n",
    "def load_weights(agent_weight_map):\n",
    "    for agent_id, agent_weights in agent_weight_map.items():\n",
    "        agent = _agent_map[agent_id]\n",
    "        agent.load_flat_params(agent_weights)\n",
    "        _solution_dim[agent_id] = np.prod(agent.flat_parameters.shape)\n",
    "        agent.reset_optimizer()\n",
    "        _agent_map[agent_id] = agent\n",
    "\n",
    "## UDP._calc_fitness\n",
    "def _calc_fitness(rollout_data: dict[AgentID, list[SampleBatch]]) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "    # Team fitness [F]:\n",
    "    # > for each agent: stack fitness metrics\n",
    "    # > [F] <- [A]x[F] (mean)\n",
    "    # >   for each fitness metric: stack rollout by episode\n",
    "    # >   [A, F] <- [E]x[A, F] (mean)\n",
    "    # >     for each episode: stack the metric over the trajectory\n",
    "    # >     [E, A, F] <- [E, A, F]x[T] (sum)\n",
    "    f = np.stack(\n",
    "        [\n",
    "            np.stack(\n",
    "                [\n",
    "                    np.stack([f[fitness_metric].sum() for f in rollout_data[agent_id]]).mean(axis=0, keepdims=False)\n",
    "                    for fitness_metric in fitness_metric_keys\n",
    "                ],\n",
    "            )\n",
    "            for agent_id in training_agents\n",
    "        ],\n",
    "    )\n",
    "    assert f.shape[-1] == len(fitness_metric_keys)  # noqa: S101\n",
    "    f = f if fitness_multiplier is None else f * fitness_multiplier[None]\n",
    "    mean_f = f.mean(axis=0, keepdims=False)\n",
    "    infos = {\n",
    "        f\"agent/{agent_id}\": {fitness_metric: agent_f[idx] for idx, fitness_metric in enumerate(fitness_metric_keys)}\n",
    "        for agent_id, agent_f in zip(training_agents, f, strict=True)\n",
    "    }\n",
    "    infos.update({fitness_metric: mean_f[idx] for idx, fitness_metric in enumerate(fitness_metric_keys)})\n",
    "    return f if _fitness_reduce_fn is None else _fitness_reduce_fn(f), infos\n",
    "\n",
    "\n",
    "\n",
    "## UDP.fitness()\n",
    "teams: dict[AgentID, MOAIMIslandInhabitant] = get_teams_for_training(batch_size=None, randomized=True)\n",
    "display(teams)\n",
    "\n",
    "training_iterations = 1\n",
    "train = True\n",
    "\n",
    "for team_id, team in teams.items():\n",
    "    _rollout_buffers = {}\n",
    "    _agent_id_map = {}\n",
    "\n",
    "    for agent in _agent_map.values():\n",
    "        # set all agents to eval mode\n",
    "        agent.train(mode=False)\n",
    "\n",
    "    agent_weight_map = {agent.name: agent.genome.flatten() for agent in team.values()}\n",
    "    load_weights(agent_weight_map)\n",
    "    for idx, agent_id in enumerate(team):\n",
    "        _agent_id_map.clear()\n",
    "        _agent_id_map.update({agent_id: idx})\n",
    "        _agent_map[agent_id].reset_optimizer()\n",
    "        _agent_map[agent_id].train(mode=True)\n",
    "\n",
    "\n",
    "    ## train\n",
    "    infos = defaultdict(list)\n",
    "    for _it in range(training_iterations):\n",
    "        # inside runner, collect rollouts then do an island training step\n",
    "        results: dict[AgentID, dict[str, SampleBatch | Any]] = runner(\n",
    "            train=train,\n",
    "            remote=False,\n",
    "            agent_map=_agent_map,\n",
    "            trainable_agents=training_agents,\n",
    "            opponent_agents=opponent_agents,\n",
    "            reward_metrics=dict.fromkeys(_agent_map, fitness_metric_keys),\n",
    "            reward_metric_gains=dict.fromkeys(_agent_map, fitness_multiplier),\n",
    "        )\n",
    "        flat = flatten_dicts(\n",
    "            {\n",
    "                f\"agent/{agent_id}/{'training' if train and 'training_stats' in res else 'rollout'}\": res[\n",
    "                    \"training_stats\" if train and \"training_stats\" in res else \"rollout_stats\"\n",
    "                ]\n",
    "                for agent_id, res in results.items()\n",
    "            },\n",
    "        )\n",
    "        for path, info in flat.items():\n",
    "            infos[path].append(info)\n",
    "\n",
    "\n",
    "    # if train:\n",
    "    #     # evaluate the agent once\n",
    "    #     for agent in _agent_map.values():\n",
    "    #         agent.train(False)\n",
    "    #     results = self.rollout_rl(\n",
    "    #         agent_map=self._agent_map,\n",
    "    #         trainable_agents=self.training_agents,\n",
    "    #         opponent_agents=self.opponent_agents,\n",
    "    #         reward_metrics=dict.fromkeys(self._agent_map, self.fitness_metric_keys),\n",
    "    #         reward_metric_gains=dict.fromkeys(self._agent_map, self.fitness_multiplier),\n",
    "    #         **kwargs,\n",
    "    #     )\n",
    "\n",
    "    #     flat = flatten_dicts(\n",
    "    #         {\n",
    "    #             f\"agent/{agent_id}/{'training' if train and 'training_stats' in res else 'rollout'}\": res[\n",
    "    #                 \"training_stats\" if train and \"training_stats\" in res else \"rollout_stats\"\n",
    "    #             ]\n",
    "    #             for agent_id, res in results.items()\n",
    "    #         },\n",
    "    #     )\n",
    "    #     for path, info in flat.items():\n",
    "    #         infos[path].append(info)\n",
    "\n",
    "    for path, info in infos.items():\n",
    "        infos[path] = np.stack(info).mean(axis=0)\n",
    "\n",
    "    # make sure we clear the buffers before each rollout\n",
    "    _rollout_buffers.clear()\n",
    "    _rollout_buffers.update(\n",
    "        {agent_id: results[agent_id][\"extra_info\"][\"rollout_buffer\"].split_by_episode() for agent_id in training_agents},\n",
    "    )\n",
    "    fitness, extra_infos = _calc_fitness(_rollout_buffers)\n",
    "    infos.update(extra_infos)\n",
    "\n",
    "    display(infos)\n",
    "    print(\"\\n\" * 4)\n",
    "    print(\"tag_score\", infos[\"tag_score\"])\n",
    "    print(\"agent/agent_0/training/total_loss\", infos[\"agent/agent_0/training/total_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
