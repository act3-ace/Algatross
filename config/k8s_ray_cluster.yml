apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  name: momarl-raycluster
spec:
  rayVersion: "2.23.0"
  enableInTreeAutoscaling: true
  autoscalerOptions:
  headGroupSpec:
    serviceType: ClusterIP # Options are ClusterIP, NodePort, and LoadBalancer
    rayStartParams:
      num-cpus: "7" # cpus to give entirely to ray for tasks
      ray-debugger-external: "false"
      dashboard-host: "0.0.0.0"
    template: # Pod template
      metadata: # Pod metadata
        spec: # Pod spec
          containers:
            - name: ray-head
              image: reg.git.act3-ace.com/stalwart/ascension/mo-marl/development/kuberay-lite:v1.0.0-dev.7
              resources:
                limits:
                  cpu: 8
                  memory: 128Gi
                requests:
                  cpu: 8
                  memory: 128Gi
              # Keep this preStop hook in each Ray container config.
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              ports: # Optional service port overrides
                - containerPort: 6379
                  name: gcs
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
  workerGroupSpecs:
    - groupName: cpu-group
      replicas: 1
      minReplicas: 2
      maxReplicas: 2
      rayStartParams:
        num-cpus: "64"
        ray-debugger-external: "false"
      template: # Pod template
        spec:
          priorityClassName: spot
          # runtimeClassName: nvidia
          volumes:
            - name: petabyte-data
              nfs:
                path: /ifs/act3/data
                server: petabyte.act3-ace.ai
            - emptyDir:
                medium: Memory
                sizeLimit: 4Gi
              name: dshm
          containers:
            - name: actor-test
              image: reg.git.act3-ace.com/stalwart/ascension/mo-marl/development/kuberay-lite:v1.0.0-dev.7
              resources:
                limits:
                  cpu: "64"
                  memory: 250Gi
                  nvidia.com/gpu: "0"
                requests:
                  cpu: "64"
                  memory: 250Gi
                  nvidia.com/gpu: "0"
              volumeMounts:
                - mountPath: /data/petabyte
                  name: petabyte-data
                - mountPath: /dev/shm
                  name: dshm
              # command:
              # - /bin/sh
              # - -c
              # args:
              # - echo $JOB_COMPLETION_INDEX; uv sync --frozen; python scripts/training.py
